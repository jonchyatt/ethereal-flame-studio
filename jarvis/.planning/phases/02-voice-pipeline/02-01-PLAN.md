---
phase: 02-voice-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/api/jarvis/stt/route.ts
  - src/lib/jarvis/voice/DeepgramClient.ts
  - src/lib/jarvis/voice/types.ts
  - src/lib/jarvis/voice/index.ts
autonomous: true

user_setup:
  - service: deepgram
    why: "Speech-to-text transcription"
    env_vars:
      - name: DEEPGRAM_API_KEY
        source: "Deepgram Console -> Settings -> API Keys -> Create Key"
    account_required: true
    signup_url: "https://console.deepgram.com/signup"

must_haves:
  truths:
    - "Audio from MicrophoneCapture is sent to Deepgram and returns transcript text"
    - "Interim transcripts appear within 200ms of speech"
    - "Final transcript is accurate to what was spoken"
    - "API key is never exposed in browser DevTools"
  artifacts:
    - path: "src/app/api/jarvis/stt/route.ts"
      provides: "Server-side Deepgram proxy endpoint"
      exports: ["GET"]
    - path: "src/lib/jarvis/voice/DeepgramClient.ts"
      provides: "Browser-side STT WebSocket handler"
      exports: ["DeepgramClient"]
    - path: "src/lib/jarvis/voice/types.ts"
      provides: "Voice pipeline type definitions"
      exports: ["TranscriptResult", "STTConfig"]
  key_links:
    - from: "src/lib/jarvis/voice/DeepgramClient.ts"
      to: "/api/jarvis/stt"
      via: "WebSocket connection"
      pattern: "new WebSocket.*api/jarvis/stt"
    - from: "src/app/api/jarvis/stt/route.ts"
      to: "Deepgram API"
      via: "Server-side SDK"
      pattern: "createClient.*DEEPGRAM_API_KEY"
---

<objective>
Integrate Deepgram streaming speech-to-text with secure backend proxy

Purpose: Enable real-time transcription of user speech with interim results for responsive UX. API key stays server-side for security.

Output: DeepgramClient class that connects browser audio to Deepgram via backend WebSocket proxy, returning transcript events.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@jarvis/.planning/PROJECT.md
@jarvis/.planning/ROADMAP.md
@jarvis/.planning/STATE.md
@jarvis/.planning/phases/02-voice-pipeline/02-RESEARCH.md
@jarvis/.planning/phases/02-voice-pipeline/02-CONTEXT.md

# Phase 1 deliverables this plan builds on
@src/lib/jarvis/audio/MicrophoneCapture.ts
@src/lib/jarvis/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create voice pipeline types</name>
  <files>src/lib/jarvis/voice/types.ts, src/lib/jarvis/voice/index.ts</files>
  <action>
Create type definitions for voice pipeline in src/lib/jarvis/voice/types.ts:

```typescript
// STT transcript result from Deepgram
export interface TranscriptResult {
  transcript: string;
  isFinal: boolean;      // Max accuracy for this segment
  speechFinal: boolean;  // User finished speaking (utterance complete)
  confidence: number;
  words?: Array<{
    word: string;
    start: number;
    end: number;
    confidence: number;
  }>;
}

// STT configuration options
export interface STTConfig {
  model?: string;          // Default: 'nova-3'
  language?: string;       // Default: 'en-US'
  interimResults?: boolean; // Default: true
  smartFormat?: boolean;   // Default: true
}

// Event callbacks for DeepgramClient
export interface STTCallbacks {
  onTranscript: (result: TranscriptResult) => void;
  onError: (error: Error) => void;
  onOpen?: () => void;
  onClose?: () => void;
}

// Voice pipeline state (extends JarvisState in 02-03)
export type VoicePipelineState =
  | 'idle'        // Waiting for user action
  | 'listening'   // Recording audio, sending to STT
  | 'processing'  // STT complete, generating response
  | 'speaking'    // TTS audio playing
  | 'error';      // Something went wrong
```

Create barrel export in src/lib/jarvis/voice/index.ts:
```typescript
export * from './types';
export { DeepgramClient } from './DeepgramClient';
```
  </action>
  <verify>Files exist and TypeScript compiles: `npx tsc --noEmit src/lib/jarvis/voice/types.ts`</verify>
  <done>Voice pipeline types defined and exported</done>
</task>

<task type="auto">
  <name>Task 2: Create Deepgram WebSocket proxy API route</name>
  <files>src/app/api/jarvis/stt/route.ts</files>
  <action>
Create server-side WebSocket proxy for Deepgram in src/app/api/jarvis/stt/route.ts.

**Important architectural note:** Next.js App Router does not support WebSocket upgrades in route handlers. Instead, use a different approach:

**Option A (Recommended for Phase 2):** Server-Sent Events (SSE) with POST for audio chunks
- Client POSTs audio chunks to endpoint
- Server maintains Deepgram connection per session
- Server streams transcripts back via SSE
- **Deployment note:** In-memory session Map works for local dev and single-instance deployment. For serverless (Vercel), each request may hit different instance. Workaround: either self-host on persistent server, or switch to Option B.

**Option B:** Use Deepgram's browser SDK with short-lived API tokens (less secure but simpler)
- Create /api/jarvis/stt/token endpoint that returns short-lived token (30s TTL)
- Browser SDK connects directly to Deepgram with temporary token
- More requests for tokens but simpler serverless deployment

Implement Option A pattern:

```typescript
// src/app/api/jarvis/stt/route.ts
import { createClient, LiveTranscriptionEvents } from '@deepgram/sdk';

// Store active connections by session ID
const sessions = new Map<string, any>();

// GET: Create SSE stream for receiving transcripts
export async function GET(request: Request) {
  const { searchParams } = new URL(request.url);
  const sessionId = searchParams.get('sessionId');

  if (!sessionId) {
    return new Response('Missing sessionId', { status: 400 });
  }

  // Create Deepgram connection
  const deepgram = createClient(process.env.DEEPGRAM_API_KEY!);
  const connection = deepgram.listen.live({
    model: 'nova-3',
    language: 'en-US',
    smart_format: true,
    interim_results: true,
    utterance_end_ms: 1000,
    vad_events: true,
  });

  // Create SSE stream
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();
  const encoder = new TextEncoder();

  // Send transcript events to client
  connection.on(LiveTranscriptionEvents.Transcript, (data) => {
    const result = {
      transcript: data.channel.alternatives[0]?.transcript || '',
      isFinal: data.is_final,
      speechFinal: data.speech_final,
      confidence: data.channel.alternatives[0]?.confidence || 0,
    };
    writer.write(encoder.encode(`data: ${JSON.stringify(result)}\n\n`));
  });

  connection.on(LiveTranscriptionEvents.Error, (error) => {
    writer.write(encoder.encode(`data: ${JSON.stringify({ error: error.message })}\n\n`));
  });

  connection.on(LiveTranscriptionEvents.Close, () => {
    writer.close();
    sessions.delete(sessionId);
  });

  // Store connection for POST requests
  sessions.set(sessionId, { connection, writer });

  return new Response(stream.readable, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}

// POST: Receive audio chunks and forward to Deepgram
export async function POST(request: Request) {
  const { searchParams } = new URL(request.url);
  const sessionId = searchParams.get('sessionId');

  const session = sessions.get(sessionId || '');
  if (!session) {
    return new Response('Session not found', { status: 404 });
  }

  const audioData = await request.arrayBuffer();
  session.connection.send(Buffer.from(audioData));

  return new Response('OK', { status: 200 });
}

// DELETE: Close session
export async function DELETE(request: Request) {
  const { searchParams } = new URL(request.url);
  const sessionId = searchParams.get('sessionId');

  const session = sessions.get(sessionId || '');
  if (session) {
    session.connection.finish();
    sessions.delete(sessionId || '');
  }

  return new Response('OK', { status: 200 });
}
```

Add DEEPGRAM_API_KEY to .env.local (create if not exists):
```
DEEPGRAM_API_KEY=your_key_here
```
  </action>
  <verify>
1. `npm run build` succeeds (no TypeScript errors)
2. Start dev server, verify `/api/jarvis/stt` returns 400 without sessionId
  </verify>
  <done>Deepgram proxy endpoint accepts audio, returns transcripts via SSE, API key never in browser</done>
</task>

<task type="auto">
  <name>Task 3: Create DeepgramClient browser class</name>
  <files>src/lib/jarvis/voice/DeepgramClient.ts</files>
  <action>
Create browser-side STT client that:
1. Connects to /api/jarvis/stt via SSE for receiving transcripts
2. Uses MediaRecorder to capture audio from MicrophoneCapture's MediaStream
3. POSTs audio chunks to server
4. Emits transcript events via callbacks

```typescript
// src/lib/jarvis/voice/DeepgramClient.ts
import { STTCallbacks, STTConfig, TranscriptResult } from './types';

export class DeepgramClient {
  private sessionId: string;
  private eventSource: EventSource | null = null;
  private mediaRecorder: MediaRecorder | null = null;
  private callbacks: STTCallbacks;
  private config: STTConfig;
  private isActive = false;

  constructor(callbacks: STTCallbacks, config: STTConfig = {}) {
    this.sessionId = crypto.randomUUID();
    this.callbacks = callbacks;
    this.config = {
      model: 'nova-3',
      language: 'en-US',
      interimResults: true,
      smartFormat: true,
      ...config,
    };
  }

  /**
   * Start STT session with audio from MediaStream
   */
  async start(mediaStream: MediaStream): Promise<void> {
    if (this.isActive) {
      console.warn('[DeepgramClient] Already active');
      return;
    }

    this.isActive = true;

    // Connect SSE for receiving transcripts
    this.eventSource = new EventSource(
      `/api/jarvis/stt?sessionId=${this.sessionId}`
    );

    this.eventSource.onopen = () => {
      console.log('[DeepgramClient] SSE connected');
      this.callbacks.onOpen?.();
    };

    this.eventSource.onmessage = (event) => {
      try {
        const data = JSON.parse(event.data);
        if (data.error) {
          this.callbacks.onError(new Error(data.error));
        } else {
          this.callbacks.onTranscript(data as TranscriptResult);
        }
      } catch (e) {
        console.error('[DeepgramClient] Failed to parse message:', e);
      }
    };

    this.eventSource.onerror = (error) => {
      console.error('[DeepgramClient] SSE error:', error);
      this.callbacks.onError(new Error('SSE connection error'));
    };

    // Set up MediaRecorder for audio capture
    const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
      ? 'audio/webm;codecs=opus'
      : 'audio/webm';

    this.mediaRecorder = new MediaRecorder(mediaStream, { mimeType });

    this.mediaRecorder.ondataavailable = async (event) => {
      if (event.data.size > 0 && this.isActive) {
        // POST audio chunk to server
        try {
          await fetch(`/api/jarvis/stt?sessionId=${this.sessionId}`, {
            method: 'POST',
            body: event.data,
          });
        } catch (e) {
          console.error('[DeepgramClient] Failed to send audio:', e);
        }
      }
    };

    // Start recording with 250ms chunks (optimal for streaming)
    this.mediaRecorder.start(250);
    console.log('[DeepgramClient] Recording started');
  }

  /**
   * Stop STT session
   */
  async stop(): Promise<void> {
    if (!this.isActive) return;

    this.isActive = false;

    // Stop MediaRecorder
    if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {
      this.mediaRecorder.stop();
      this.mediaRecorder = null;
    }

    // Close server session
    try {
      await fetch(`/api/jarvis/stt?sessionId=${this.sessionId}`, {
        method: 'DELETE',
      });
    } catch (e) {
      console.error('[DeepgramClient] Failed to close session:', e);
    }

    // Close SSE
    if (this.eventSource) {
      this.eventSource.close();
      this.eventSource = null;
      this.callbacks.onClose?.();
    }

    console.log('[DeepgramClient] Stopped');
  }

  /**
   * Check if client is currently active
   */
  getIsActive(): boolean {
    return this.isActive;
  }
}
```

Update barrel export to include DeepgramClient.
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
2. Export works: `grep "DeepgramClient" src/lib/jarvis/voice/index.ts`
  </verify>
  <done>DeepgramClient connects browser audio to server proxy, receives transcript events</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Install dependencies:**
   ```bash
   npm install @deepgram/sdk
   ```

2. **Build check:**
   ```bash
   npm run build
   ```

3. **Manual verification (requires API key):**
   - Add DEEPGRAM_API_KEY to .env.local
   - Start dev server
   - In browser console, test DeepgramClient with MicrophoneCapture's MediaStream
   - Verify transcripts appear in console

4. **API key security check:**
   - Open DevTools Network tab
   - Verify no requests contain API key in URL or headers
   - Verify SSE connection goes to /api/jarvis/stt, not api.deepgram.com
</verification>

<success_criteria>
- DeepgramClient.start(mediaStream) begins sending audio to server
- Transcripts arrive via SSE within 200ms of speech
- DeepgramClient.stop() cleanly closes all connections
- API key visible only in server logs, never in browser
- No TypeScript errors in build
</success_criteria>

<output>
After completion, create `jarvis/.planning/phases/02-voice-pipeline/02-01-SUMMARY.md`
</output>
