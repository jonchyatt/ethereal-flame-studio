---
phase: 02-voice-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - src/lib/jarvis/voice/VoicePipeline.ts
  - src/lib/jarvis/stores/jarvisStore.ts
  - src/lib/jarvis/types.ts
  - src/app/jarvis/page.tsx
  - src/lib/jarvis/voice/index.ts
autonomous: false

must_haves:
  truths:
    - "User holds PTT, speaks, releases, and hears echo response within 300ms"
    - "Orb transitions: idle -> listening -> processing -> speaking -> idle"
    - "Transcription (if shown) appears while user speaks"
    - "Pipeline handles errors gracefully with spoken feedback"
  artifacts:
    - path: "src/lib/jarvis/voice/VoicePipeline.ts"
      provides: "Voice pipeline state machine orchestrating STT->TTS flow"
      exports: ["VoicePipeline"]
    - path: "src/lib/jarvis/stores/jarvisStore.ts"
      provides: "Extended store with voice pipeline state"
      exports: ["useJarvisStore"]
    - path: "src/app/jarvis/page.tsx"
      provides: "Jarvis page with integrated voice pipeline"
      contains: "VoicePipeline"
  key_links:
    - from: "src/lib/jarvis/voice/VoicePipeline.ts"
      to: "DeepgramClient"
      via: "STT callbacks"
      pattern: "DeepgramClient.*onTranscript"
    - from: "src/lib/jarvis/voice/VoicePipeline.ts"
      to: "ElevenLabsClient"
      via: "TTS speak call"
      pattern: "ElevenLabsClient.*speak"
    - from: "src/lib/jarvis/voice/VoicePipeline.ts"
      to: "jarvisStore"
      via: "State updates"
      pattern: "setOrbState|setPipelineState"
---

<objective>
Implement voice pipeline state machine with turn-taking and echo test

Purpose: Orchestrate the complete voice flow: user speaks -> STT transcribes -> TTS responds with echo. This validates the entire pipeline works before adding intelligence in Phase 3.

Output: VoicePipeline class that coordinates PTT, STT, TTS, and orb state transitions. Echo test proves latency target is achievable.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@jarvis/.planning/PROJECT.md
@jarvis/.planning/ROADMAP.md
@jarvis/.planning/STATE.md
@jarvis/.planning/phases/02-voice-pipeline/02-RESEARCH.md
@jarvis/.planning/phases/02-voice-pipeline/02-CONTEXT.md

# Dependencies from this phase (02-01, 02-02)
@src/lib/jarvis/voice/DeepgramClient.ts
@src/lib/jarvis/voice/ElevenLabsClient.ts
@src/lib/jarvis/voice/AudioPlayer.ts
@src/lib/jarvis/voice/types.ts

# Phase 1 deliverables
@src/lib/jarvis/audio/MicrophoneCapture.ts
@src/lib/jarvis/stores/jarvisStore.ts
@src/lib/jarvis/types.ts
@src/app/jarvis/page.tsx
@src/components/jarvis/PushToTalk.tsx
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend store with voice pipeline state</name>
  <files>src/lib/jarvis/types.ts, src/lib/jarvis/stores/jarvisStore.ts</files>
  <action>
Extend the Jarvis store to include voice pipeline state for orchestration.

Update src/lib/jarvis/types.ts - add voice pipeline state:

```typescript
// Add to existing types

// Voice pipeline states
export type VoicePipelineState =
  | 'idle'        // Waiting for user action
  | 'listening'   // Recording audio, sending to STT
  | 'processing'  // STT complete, generating response
  | 'speaking'    // TTS audio playing
  | 'error';      // Something went wrong

// Extended Jarvis state
export interface JarvisState {
  // Existing fields...
  orbState: OrbState;
  isAudioPermissionGranted: boolean;
  isCapturing: boolean;
  audioLevel: number;
  importance: number;
  stateColors: Record<OrbState, [number, number, number]>;

  // Voice pipeline additions
  pipelineState: VoicePipelineState;
  currentTranscript: string;      // Live interim transcript
  finalTranscript: string;        // Complete utterance after PTT release
  lastResponse: string;           // Last TTS response text
  error: string | null;           // Current error message
  showTranscript: boolean;        // Whether to display transcript UI
}

// Extended actions
export interface JarvisActions {
  // Existing actions...
  setOrbState: (state: OrbState) => void;
  setAudioPermissionGranted: (granted: boolean) => void;
  setIsCapturing: (capturing: boolean) => void;
  setAudioLevel: (level: number) => void;
  setImportance: (importance: number) => void;
  setStateColor: (state: OrbState, color: [number, number, number]) => void;

  // Voice pipeline actions
  setPipelineState: (state: VoicePipelineState) => void;
  setCurrentTranscript: (transcript: string) => void;
  setFinalTranscript: (transcript: string) => void;
  setLastResponse: (response: string) => void;
  setError: (error: string | null) => void;
  setShowTranscript: (show: boolean) => void;
  resetPipeline: () => void;
}
```

Update src/lib/jarvis/stores/jarvisStore.ts:

```typescript
import { create } from 'zustand';
import {
  JarvisState,
  JarvisActions,
  OrbState,
  VoicePipelineState,
  DEFAULT_STATE_COLORS,
} from '../types';

type JarvisStore = JarvisState & JarvisActions;

export const useJarvisStore = create<JarvisStore>((set) => ({
  // Existing initial state
  orbState: 'idle',
  isAudioPermissionGranted: false,
  isCapturing: false,
  audioLevel: 0,
  importance: 0,
  stateColors: DEFAULT_STATE_COLORS,

  // Voice pipeline initial state
  pipelineState: 'idle',
  currentTranscript: '',
  finalTranscript: '',
  lastResponse: '',
  error: null,
  showTranscript: false,

  // Existing actions
  setOrbState: (orbState: OrbState) => set({ orbState }),
  setAudioPermissionGranted: (granted: boolean) =>
    set({ isAudioPermissionGranted: granted }),
  setIsCapturing: (capturing: boolean) => set({ isCapturing: capturing }),
  setAudioLevel: (level: number) =>
    set({ audioLevel: Math.max(0, Math.min(1, level)) }),
  setImportance: (importance: number) =>
    set({ importance: Math.max(0, Math.min(1, importance)) }),
  setStateColor: (state: OrbState, color: [number, number, number]) =>
    set((s) => ({
      stateColors: { ...s.stateColors, [state]: color },
    })),

  // Voice pipeline actions
  setPipelineState: (pipelineState: VoicePipelineState) => set({ pipelineState }),
  setCurrentTranscript: (currentTranscript: string) => set({ currentTranscript }),
  setFinalTranscript: (finalTranscript: string) => set({ finalTranscript }),
  setLastResponse: (lastResponse: string) => set({ lastResponse }),
  setError: (error: string | null) => set({ error }),
  setShowTranscript: (showTranscript: boolean) => set({ showTranscript }),
  resetPipeline: () =>
    set({
      pipelineState: 'idle',
      currentTranscript: '',
      finalTranscript: '',
      error: null,
    }),
}));
```
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
2. Store exports extended types
  </verify>
  <done>Store extended with voice pipeline state and actions</done>
</task>

<task type="auto">
  <name>Task 2: Create VoicePipeline state machine</name>
  <files>src/lib/jarvis/voice/VoicePipeline.ts, src/lib/jarvis/voice/index.ts</files>
  <action>
Create the voice pipeline orchestrator that coordinates STT, TTS, and state.

```typescript
// src/lib/jarvis/voice/VoicePipeline.ts
import { useJarvisStore } from '../stores/jarvisStore';
import { MicrophoneCapture } from '../audio/MicrophoneCapture';
import { DeepgramClient } from './DeepgramClient';
import { ElevenLabsClient } from './ElevenLabsClient';
import { VoicePipelineState } from '../types';

export interface VoicePipelineConfig {
  // Echo test response generator (Phase 2)
  // In Phase 3, this becomes Claude API call
  generateResponse?: (transcript: string) => string;
}

export class VoicePipeline {
  private microphone: MicrophoneCapture;
  private sttClient: DeepgramClient | null = null;
  private ttsClient: ElevenLabsClient;
  private config: VoicePipelineConfig;

  private isInitialized = false;
  private mediaStream: MediaStream | null = null;

  constructor(config: VoicePipelineConfig = {}) {
    this.microphone = MicrophoneCapture.getInstance();
    this.config = {
      // Default echo response for Phase 2
      generateResponse: (transcript) => `You said: ${transcript}`,
      ...config,
    };

    // Initialize TTS client
    this.ttsClient = new ElevenLabsClient({
      onStart: () => {
        this.setState('speaking');
        useJarvisStore.getState().setOrbState('speaking');
      },
      onEnd: () => {
        this.setState('idle');
        useJarvisStore.getState().setOrbState('idle');
        console.log('[VoicePipeline] Turn complete');
      },
      onError: (error) => {
        console.error('[VoicePipeline] TTS error:', error);
        this.handleError('Speech synthesis failed');
      },
    });
  }

  /**
   * Initialize pipeline (call after user grants permission)
   */
  async initialize(): Promise<boolean> {
    if (this.isInitialized) return true;

    // Check if microphone permission is granted
    if (!this.microphone.hasPermission()) {
      const granted = await this.microphone.requestPermission();
      if (!granted) {
        this.handleError('Microphone permission required');
        return false;
      }
    }

    this.isInitialized = true;
    console.log('[VoicePipeline] Initialized');
    return true;
  }

  /**
   * Start listening (called when PTT pressed)
   */
  async startListening(): Promise<void> {
    if (!this.isInitialized) {
      const ready = await this.initialize();
      if (!ready) return;
    }

    const state = useJarvisStore.getState();

    // Interrupt TTS if speaking (barge-in)
    if (state.pipelineState === 'speaking') {
      console.log('[VoicePipeline] Barge-in: interrupting TTS');
      this.ttsClient.stop();
    }

    // Reset transcript state
    state.setCurrentTranscript('');
    state.setFinalTranscript('');
    state.setError(null);

    // Transition to listening
    this.setState('listening');
    state.setOrbState('listening');

    // Start microphone
    this.microphone.start();

    // Get MediaStream for STT
    // Note: We need to access the MediaStream from MicrophoneCapture
    // This requires adding a getter to MicrophoneCapture (see below)
    this.mediaStream = this.microphone.getMediaStream();

    if (!this.mediaStream) {
      this.handleError('No audio stream available');
      return;
    }

    // Create and start STT client
    this.sttClient = new DeepgramClient({
      onTranscript: (result) => {
        const store = useJarvisStore.getState();

        // Update current transcript
        store.setCurrentTranscript(result.transcript);

        // If this is a final transcript segment, accumulate it
        if (result.isFinal && result.transcript) {
          const current = store.finalTranscript;
          const updated = current ? `${current} ${result.transcript}` : result.transcript;
          store.setFinalTranscript(updated);
        }

        console.log(`[VoicePipeline] STT: [${result.isFinal ? 'FINAL' : 'interim'}] "${result.transcript}"`);
      },
      onError: (error) => {
        console.error('[VoicePipeline] STT error:', error);
        this.handleError('Speech recognition failed');
      },
      onOpen: () => {
        console.log('[VoicePipeline] STT connected');
      },
      onClose: () => {
        console.log('[VoicePipeline] STT disconnected');
      },
    });

    await this.sttClient.start(this.mediaStream);
    console.log('[VoicePipeline] Listening started');
  }

  /**
   * Stop listening and process (called when PTT released)
   */
  async stopListening(): Promise<void> {
    if (useJarvisStore.getState().pipelineState !== 'listening') {
      return;
    }

    // Stop STT
    if (this.sttClient) {
      await this.sttClient.stop();
      this.sttClient = null;
    }

    // Stop microphone
    this.microphone.stop();

    const state = useJarvisStore.getState();
    const transcript = state.finalTranscript || state.currentTranscript;

    console.log(`[VoicePipeline] Final transcript: "${transcript}"`);

    if (!transcript.trim()) {
      console.log('[VoicePipeline] No speech detected');
      this.setState('idle');
      state.setOrbState('idle');
      return;
    }

    // Transition to processing
    this.setState('processing');
    state.setOrbState('thinking');

    // Generate response (echo in Phase 2, Claude in Phase 3)
    const response = this.config.generateResponse!(transcript);
    state.setLastResponse(response);

    console.log(`[VoicePipeline] Response: "${response}"`);

    // Speak response
    await this.ttsClient.speak(response);
  }

  /**
   * Cancel current operation
   */
  cancel(): void {
    // Stop STT if active
    if (this.sttClient) {
      this.sttClient.stop();
      this.sttClient = null;
    }

    // Stop microphone if capturing
    this.microphone.stop();

    // Stop TTS if speaking
    this.ttsClient.stop();

    // Reset state
    useJarvisStore.getState().resetPipeline();
    useJarvisStore.getState().setOrbState('idle');

    console.log('[VoicePipeline] Cancelled');
  }

  /**
   * Set pipeline state and sync with orb
   */
  private setState(state: VoicePipelineState): void {
    useJarvisStore.getState().setPipelineState(state);
  }

  /**
   * Handle error state
   */
  private handleError(message: string): void {
    const state = useJarvisStore.getState();
    state.setError(message);
    state.setPipelineState('error');
    state.setOrbState('idle');

    // Stop any active processes
    this.sttClient?.stop();
    this.microphone.stop();

    // Speak error message
    this.ttsClient.speak("I didn't catch that. Please try again.");

    // Auto-recover after error speech
    setTimeout(() => {
      state.resetPipeline();
    }, 3000);
  }

  /**
   * Get current pipeline state
   */
  getState(): VoicePipelineState {
    return useJarvisStore.getState().pipelineState;
  }

  /**
   * Check if pipeline is busy
   */
  isBusy(): boolean {
    const state = this.getState();
    return state !== 'idle' && state !== 'error';
  }

  /**
   * Cleanup resources
   */
  cleanup(): void {
    this.cancel();
    this.ttsClient.cleanup();
    this.isInitialized = false;
    console.log('[VoicePipeline] Cleanup complete');
  }
}
```

**Important:** MicrophoneCapture needs a getter for MediaStream. Add to src/lib/jarvis/audio/MicrophoneCapture.ts:

```typescript
/**
 * Get the current MediaStream (for use with STT)
 */
getMediaStream(): MediaStream | null {
  return this.mediaStream;
}
```

Update barrel export in src/lib/jarvis/voice/index.ts:
```typescript
export * from './types';
export { DeepgramClient } from './DeepgramClient';
export { AudioPlayer } from './AudioPlayer';
export { ElevenLabsClient } from './ElevenLabsClient';
export { VoicePipeline } from './VoicePipeline';
```
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
2. VoicePipeline exports from index
3. MicrophoneCapture.getMediaStream() exists
  </verify>
  <done>VoicePipeline orchestrates STT->TTS flow with state machine</done>
</task>

<task type="auto">
  <name>Task 3: Integrate VoicePipeline into Jarvis page</name>
  <files>src/app/jarvis/page.tsx</files>
  <action>
Update the Jarvis page to use VoicePipeline for PTT handling.

Replace current PTT handlers with VoicePipeline integration:

```typescript
'use client';

import { useEffect, useRef, useState } from 'react';
import { useJarvisStore } from '@/lib/jarvis/stores/jarvisStore';
import { MicrophoneCapture } from '@/lib/jarvis/audio/MicrophoneCapture';
import { VoicePipeline } from '@/lib/jarvis/voice/VoicePipeline';
import { JarvisOrb } from '@/components/jarvis/JarvisOrb';
import { OrbStateManager } from '@/components/jarvis/OrbStateManager';
import { PermissionPrompt } from '@/components/jarvis/PermissionPrompt';
import { PushToTalk } from '@/components/jarvis/PushToTalk';

export default function JarvisPage() {
  const {
    isAudioPermissionGranted,
    pipelineState,
    currentTranscript,
    showTranscript,
  } = useJarvisStore();

  const pipelineRef = useRef<VoicePipeline | null>(null);
  const [permissionState, setPermissionState] = useState<'checking' | 'prompt' | 'granted' | 'denied'>('checking');

  // Check initial permission state
  useEffect(() => {
    const checkPermission = async () => {
      if (!MicrophoneCapture.isSupported()) {
        setPermissionState('denied');
        return;
      }

      const state = await MicrophoneCapture.checkPermission();
      if (state === 'granted') {
        setPermissionState('granted');
        const mic = MicrophoneCapture.getInstance();
        await mic.requestPermission();
      } else if (state === 'denied') {
        setPermissionState('denied');
      } else {
        setPermissionState('prompt');
      }
    };

    checkPermission();
  }, []);

  // Initialize pipeline after permission granted
  useEffect(() => {
    if (isAudioPermissionGranted && !pipelineRef.current) {
      pipelineRef.current = new VoicePipeline();
      pipelineRef.current.initialize();
    }

    return () => {
      if (pipelineRef.current) {
        pipelineRef.current.cleanup();
        pipelineRef.current = null;
      }
    };
  }, [isAudioPermissionGranted]);

  // Handle permission request
  const handlePermissionRequest = async () => {
    const mic = MicrophoneCapture.getInstance();
    const granted = await mic.requestPermission();

    if (granted) {
      setPermissionState('granted');
    } else {
      setPermissionState('denied');
    }
  };

  // PTT handlers
  const handlePTTStart = () => {
    if (pipelineRef.current) {
      pipelineRef.current.startListening();
    }
  };

  const handlePTTEnd = () => {
    if (pipelineRef.current) {
      pipelineRef.current.stopListening();
    }
  };

  // Show permission prompt if needed
  if (permissionState === 'prompt' || permissionState === 'denied') {
    return (
      <div className="relative h-dvh w-full bg-black overflow-hidden flex items-center justify-center">
        <PermissionPrompt
          onRequestPermission={handlePermissionRequest}
          isDenied={permissionState === 'denied'}
        />
      </div>
    );
  }

  // Show loading while checking
  if (permissionState === 'checking') {
    return (
      <div className="relative h-dvh w-full bg-black overflow-hidden flex items-center justify-center">
        <div className="text-white/50">Initializing...</div>
      </div>
    );
  }

  return (
    <div className="relative h-dvh w-full bg-black overflow-hidden">
      {/* Orb visualization */}
      <div className="absolute inset-0 flex items-center justify-center">
        <JarvisOrb />
        <OrbStateManager />
      </div>

      {/* Transcript display (optional) */}
      {showTranscript && currentTranscript && (
        <div className="absolute top-8 left-1/2 -translate-x-1/2 max-w-md px-4">
          <div className="bg-black/60 backdrop-blur-sm rounded-lg px-4 py-2 text-white/80 text-center">
            {currentTranscript}
          </div>
        </div>
      )}

      {/* Pipeline state indicator */}
      <div className="absolute bottom-24 left-1/2 -translate-x-1/2">
        <div className="text-white/40 text-sm capitalize">
          {pipelineState === 'idle' ? 'Ready' : pipelineState}
        </div>
      </div>

      {/* Push to Talk button */}
      <div className="absolute bottom-8 left-1/2 -translate-x-1/2">
        <PushToTalk onPTTStart={handlePTTStart} onPTTEnd={handlePTTEnd} />
      </div>
    </div>
  );
}
```

The PushToTalk component already handles mouse/touch/keyboard events. We just need to wire it to VoicePipeline instead of MicrophoneCapture directly.
  </action>
  <verify>
1. `npm run build` succeeds
2. Start dev server, /jarvis loads without errors
3. Orb visible, PTT button functional
  </verify>
  <done>Jarvis page integrated with VoicePipeline for voice interaction</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 4: Verify complete voice pipeline</name>
  <what-built>
Complete voice pipeline: user speaks via PTT, speech is transcribed by Deepgram, echo response is synthesized by ElevenLabs, orb animates through all states.
  </what-built>
  <how-to-verify>
**Prerequisites:**
- DEEPGRAM_API_KEY in .env.local
- ELEVENLABS_API_KEY in .env.local

**Test procedure:**
1. Start dev server: `npm run dev`
2. Navigate to http://localhost:3000/jarvis
3. Grant microphone permission when prompted
4. Click and hold the push-to-talk button
5. Speak clearly: "Hello Jarvis, can you hear me?"
6. Release the button

**Expected behavior:**
- Orb transitions: blue (idle) -> cyan (listening) -> amber (thinking) -> orange (speaking) -> blue (idle)
- Console shows: STT transcription appearing in real-time
- Console shows: "Response: You said: Hello Jarvis, can you hear me?"
- Audio plays: TTS voice speaks the echo response
- Orb pulses in sync with TTS audio

**Latency check:**
- From button release to first audio, should be < 500ms (300ms target, some buffer for dev)
- Check console for timing logs

**Error handling test:**
1. Hold PTT but don't speak
2. Release - should say "I didn't catch that"
3. Orb should return to idle

**Barge-in test (optional):**
1. Let TTS start speaking
2. Press PTT while speaking
3. TTS should stop, orb should transition to listening
  </how-to-verify>
  <resume-signal>
Type "approved" if voice pipeline works end-to-end.
If issues found, describe specific failure (e.g., "STT returns empty transcript" or "TTS never plays").
  </resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Build check:**
   ```bash
   npm run build
   ```

2. **End-to-end test:**
   - PTT press -> listening state -> orb cyan
   - Speak -> transcript appears (if enabled)
   - PTT release -> processing state -> orb amber
   - Response generated -> speaking state -> orb orange + audio
   - Audio ends -> idle state -> orb blue

3. **Latency measurement:**
   - Console should log timestamps for each stage
   - Total latency from PTT release to first audio byte

4. **Error recovery:**
   - Empty speech -> graceful "didn't catch that"
   - Network error -> error state -> auto-recover
</verification>

<success_criteria>
- Complete voice turn works: speak -> hear response
- Orb correctly shows all states (idle, listening, thinking, speaking)
- Transcription appears in real-time during speech
- Latency from speech end to audio start is measurable (target < 300ms)
- Barge-in interrupts TTS when PTT pressed
- Errors handled gracefully with spoken feedback
- No TypeScript errors in build
</success_criteria>

<output>
After completion, create `jarvis/.planning/phases/02-voice-pipeline/02-03-SUMMARY.md`
</output>
