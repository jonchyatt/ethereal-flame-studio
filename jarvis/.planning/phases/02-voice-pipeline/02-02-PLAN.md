---
phase: 02-voice-pipeline
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/jarvis/voice/SpeechClient.ts
  - src/lib/jarvis/voice/types.ts
  - src/lib/jarvis/voice/index.ts
autonomous: true

user_setup: []  # No API keys needed - uses browser SpeechSynthesis

must_haves:
  truths:
    - "Text sent to TTS plays as speech in browser"
    - "Speech starts immediately (no network latency)"
    - "Orb shows speaking state during playback"
    - "No API keys required"
  artifacts:
    - path: "src/lib/jarvis/voice/SpeechClient.ts"
      provides: "Browser-side TTS using Web Speech API"
      exports: ["SpeechClient"]
  key_links:
    - from: "src/lib/jarvis/voice/SpeechClient.ts"
      to: "window.speechSynthesis"
      via: "Web Speech API"
      pattern: "speechSynthesis.speak"
    - from: "src/lib/jarvis/voice/SpeechClient.ts"
      to: "jarvisStore"
      via: "State updates"
      pattern: "setOrbState.*speaking"
---

<objective>
Implement text-to-speech using browser's built-in SpeechSynthesis API

Purpose: Enable voice responses with zero cost and no API keys. Browser SpeechSynthesis is sufficient for Phase 2 (proving the pipeline works). Can upgrade to ElevenLabs/OpenAI TTS later when voice quality matters.

Output: SpeechClient class with same interface as future premium TTS clients (speak, stop, callbacks).

Note: Browser SpeechSynthesis doesn't expose audio stream, so orb won't pulse to voice. Orb shows "speaking" state (orange color) but doesn't have audio-reactive animation during TTS. This is acceptable for Phase 2 - upgrading TTS provider later will add orb sync.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@jarvis/.planning/PROJECT.md
@jarvis/.planning/ROADMAP.md
@jarvis/.planning/STATE.md
@jarvis/.planning/phases/02-voice-pipeline/02-CONTEXT.md

# Existing infrastructure
@src/lib/jarvis/stores/jarvisStore.ts
@src/lib/jarvis/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add TTS types</name>
  <files>src/lib/jarvis/voice/types.ts</files>
  <action>
Add TTS types to src/lib/jarvis/voice/types.ts:

```typescript
// Add to existing types.ts

// TTS configuration
export interface TTSConfig {
  voice?: string;         // Voice name (browser-dependent)
  rate?: number;          // Speech rate 0.1-10, default: 1
  pitch?: number;         // Pitch 0-2, default: 1
  volume?: number;        // Volume 0-1, default: 1
}

// TTS callbacks
export interface TTSCallbacks {
  onStart?: () => void;
  onEnd?: () => void;
  onError?: (error: Error) => void;
}
```
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
  </verify>
  <done>TTS types added</done>
</task>

<task type="auto">
  <name>Task 2: Create SpeechClient using Web Speech API</name>
  <files>src/lib/jarvis/voice/SpeechClient.ts</files>
  <action>
Create browser-side TTS client using the free, built-in Web Speech API.

```typescript
// src/lib/jarvis/voice/SpeechClient.ts
import { useJarvisStore } from '../stores/jarvisStore';
import { TTSCallbacks, TTSConfig } from './types';

export class SpeechClient {
  private synth: SpeechSynthesis;
  private callbacks: TTSCallbacks;
  private config: TTSConfig;
  private currentUtterance: SpeechSynthesisUtterance | null = null;
  private selectedVoice: SpeechSynthesisVoice | null = null;

  constructor(callbacks: TTSCallbacks = {}, config: TTSConfig = {}) {
    this.synth = window.speechSynthesis;
    this.callbacks = callbacks;
    this.config = {
      rate: 1.0,
      pitch: 1.0,
      volume: 1.0,
      ...config,
    };

    // Load voices (they load async in some browsers)
    this.loadVoices();
    if (speechSynthesis.onvoiceschanged !== undefined) {
      speechSynthesis.onvoiceschanged = () => this.loadVoices();
    }
  }

  /**
   * Load available voices and select a good default
   */
  private loadVoices(): void {
    const voices = this.synth.getVoices();

    if (voices.length === 0) return;

    // Prefer these voices for "guide" quality (calm, clear)
    const preferredVoices = [
      'Google UK English Male',
      'Google US English',
      'Microsoft David',
      'Daniel',  // macOS
      'Alex',    // macOS
    ];

    // Find first preferred voice that exists
    for (const preferred of preferredVoices) {
      const found = voices.find(v =>
        v.name.includes(preferred) || v.voiceURI.includes(preferred)
      );
      if (found) {
        this.selectedVoice = found;
        console.log(`[SpeechClient] Selected voice: ${found.name}`);
        return;
      }
    }

    // Fall back to first English voice
    const englishVoice = voices.find(v => v.lang.startsWith('en'));
    if (englishVoice) {
      this.selectedVoice = englishVoice;
      console.log(`[SpeechClient] Fallback voice: ${englishVoice.name}`);
    } else {
      // Last resort: first available voice
      this.selectedVoice = voices[0];
      console.log(`[SpeechClient] Default voice: ${voices[0].name}`);
    }
  }

  /**
   * Speak the given text
   */
  async speak(text: string): Promise<void> {
    if (!text.trim()) {
      console.warn('[SpeechClient] Empty text, skipping');
      return;
    }

    // Cancel any in-progress speech
    this.stop();

    console.log(`[SpeechClient] Speaking: "${text.substring(0, 50)}..."`);

    const utterance = new SpeechSynthesisUtterance(text);

    // Apply configuration
    if (this.selectedVoice) {
      utterance.voice = this.selectedVoice;
    }
    utterance.rate = this.config.rate ?? 1.0;
    utterance.pitch = this.config.pitch ?? 1.0;
    utterance.volume = this.config.volume ?? 1.0;

    // Set up event handlers
    utterance.onstart = () => {
      console.log('[SpeechClient] Playback started');
      useJarvisStore.getState().setOrbState('speaking');
      this.callbacks.onStart?.();
    };

    utterance.onend = () => {
      console.log('[SpeechClient] Playback ended');
      this.currentUtterance = null;
      useJarvisStore.getState().setOrbState('idle');
      this.callbacks.onEnd?.();
    };

    utterance.onerror = (event) => {
      console.error('[SpeechClient] Error:', event.error);
      this.currentUtterance = null;
      useJarvisStore.getState().setOrbState('idle');
      this.callbacks.onError?.(new Error(event.error));
    };

    this.currentUtterance = utterance;
    this.synth.speak(utterance);
  }

  /**
   * Stop current speech
   */
  stop(): void {
    if (this.synth.speaking) {
      this.synth.cancel();
      console.log('[SpeechClient] Speech cancelled');
    }
    this.currentUtterance = null;
    useJarvisStore.getState().setOrbState('idle');
  }

  /**
   * Check if currently speaking
   */
  getIsSpeaking(): boolean {
    return this.synth.speaking;
  }

  /**
   * Set volume (0-1)
   */
  setVolume(volume: number): void {
    this.config.volume = Math.max(0, Math.min(1, volume));
    // Note: Volume change applies to next utterance, not current
  }

  /**
   * Set speech rate (0.1-10, 1 = normal)
   */
  setRate(rate: number): void {
    this.config.rate = Math.max(0.1, Math.min(10, rate));
  }

  /**
   * Get available voices
   */
  getVoices(): SpeechSynthesisVoice[] {
    return this.synth.getVoices();
  }

  /**
   * Set specific voice by name
   */
  setVoice(name: string): boolean {
    const voices = this.getVoices();
    const voice = voices.find(v => v.name === name);
    if (voice) {
      this.selectedVoice = voice;
      console.log(`[SpeechClient] Voice changed to: ${voice.name}`);
      return true;
    }
    return false;
  }

  /**
   * Cleanup resources
   */
  cleanup(): void {
    this.stop();
  }
}
```
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
2. SpeechClient class defined with speak/stop methods
  </verify>
  <done>SpeechClient uses browser SpeechSynthesis API, zero cost, no API keys</done>
</task>

<task type="auto">
  <name>Task 3: Update barrel export</name>
  <files>src/lib/jarvis/voice/index.ts</files>
  <action>
Update barrel export in src/lib/jarvis/voice/index.ts:

```typescript
export * from './types';
export { DeepgramClient } from './DeepgramClient';
export { SpeechClient } from './SpeechClient';
```

Note: AudioPlayer and ElevenLabsClient are NOT included - they'll be added when upgrading to premium TTS.
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
2. SpeechClient exports from index: `grep "SpeechClient" src/lib/jarvis/voice/index.ts`
  </verify>
  <done>Voice module exports SpeechClient</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Build check:**
   ```bash
   npm run build
   ```

2. **Manual verification (no API key needed):**
   - Start dev server
   - In browser console:
     ```javascript
     const { SpeechClient } = await import('/src/lib/jarvis/voice/SpeechClient');
     const client = new SpeechClient({ onStart: () => console.log('speaking'), onEnd: () => console.log('done') });
     client.speak("Hello, I am Jarvis. How can I help you today?");
     ```
   - Verify audio plays through speakers
   - Verify "speaking" and "done" logged to console

3. **Voice selection:**
   ```javascript
   client.getVoices().map(v => v.name);  // See available voices
   client.setVoice('Google UK English Male');  // Change voice
   client.speak("Testing new voice.");
   ```
</verification>

<success_criteria>
- SpeechClient.speak(text) plays audio through browser speakers
- SpeechClient.stop() cancels current speech
- Orb transitions to speaking state during playback
- No API keys required
- No TypeScript errors in build
</success_criteria>

<future_upgrade>
When voice quality matters, add premium TTS:

1. Create `src/lib/jarvis/voice/ElevenLabsClient.ts` (or OpenAI, Google Cloud)
2. Create `src/app/api/jarvis/tts/route.ts` for API key protection
3. Create `src/lib/jarvis/voice/AudioPlayer.ts` for streaming playback with orb sync
4. Replace SpeechClient with ElevenLabsClient in VoicePipeline

The interface is the same (speak, stop, callbacks) - it's a drop-in replacement.
</future_upgrade>

<output>
After completion, create `jarvis/.planning/phases/02-voice-pipeline/02-02-SUMMARY.md`
</output>
