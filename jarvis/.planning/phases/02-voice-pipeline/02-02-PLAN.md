---
phase: 02-voice-pipeline
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/api/jarvis/tts/route.ts
  - src/lib/jarvis/voice/ElevenLabsClient.ts
  - src/lib/jarvis/voice/AudioPlayer.ts
  - src/lib/jarvis/voice/types.ts
  - src/lib/jarvis/voice/index.ts
autonomous: true

user_setup:
  - service: elevenlabs
    why: "Text-to-speech synthesis"
    env_vars:
      - name: ELEVENLABS_API_KEY
        source: "ElevenLabs -> Profile Settings -> API Keys"
    account_required: true
    signup_url: "https://elevenlabs.io/sign-up"

must_haves:
  truths:
    - "Text sent to TTS returns streaming audio that plays in browser"
    - "First audio byte plays within 150ms of request (before full response)"
    - "Audio output drives orb audioLevel for speaking animation"
    - "API key is never exposed in browser DevTools"
  artifacts:
    - path: "src/app/api/jarvis/tts/route.ts"
      provides: "Server-side TTS streaming proxy"
      exports: ["POST"]
    - path: "src/lib/jarvis/voice/ElevenLabsClient.ts"
      provides: "Browser-side TTS request handler"
      exports: ["ElevenLabsClient"]
    - path: "src/lib/jarvis/voice/AudioPlayer.ts"
      provides: "Streaming audio playback with orb sync"
      exports: ["AudioPlayer"]
  key_links:
    - from: "src/lib/jarvis/voice/ElevenLabsClient.ts"
      to: "/api/jarvis/tts"
      via: "POST request with streaming response"
      pattern: "fetch.*api/jarvis/tts"
    - from: "src/lib/jarvis/voice/AudioPlayer.ts"
      to: "jarvisStore.setAudioLevel"
      via: "AnalyserNode during playback"
      pattern: "setAudioLevel.*getAudioLevel"
---

<objective>
Integrate ElevenLabs streaming text-to-speech with audio playback synced to orb

Purpose: Enable low-latency voice responses with audio that drives the orb's speaking animation. Streaming starts audio before full response is generated.

Output: ElevenLabsClient for TTS requests, AudioPlayer for streaming playback with orb reactivity.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@jarvis/.planning/PROJECT.md
@jarvis/.planning/ROADMAP.md
@jarvis/.planning/STATE.md
@jarvis/.planning/phases/02-voice-pipeline/02-RESEARCH.md
@jarvis/.planning/phases/02-voice-pipeline/02-CONTEXT.md

# Existing infrastructure
@src/lib/jarvis/stores/jarvisStore.ts
@src/lib/jarvis/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TTS streaming API route</name>
  <files>src/app/api/jarvis/tts/route.ts</files>
  <action>
Create server-side TTS proxy that streams audio from ElevenLabs to client.

```typescript
// src/app/api/jarvis/tts/route.ts

// Voice configuration (Stuart - professional, friendly, calm)
const VOICE_ID = 'HDA9tsk27wYi3uq0fPcK';
const MODEL_ID = 'eleven_flash_v2_5'; // ~75ms inference latency

export async function POST(request: Request) {
  try {
    const { text } = await request.json();

    if (!text || typeof text !== 'string') {
      return new Response('Missing or invalid text', { status: 400 });
    }

    if (!process.env.ELEVENLABS_API_KEY) {
      return new Response('ELEVENLABS_API_KEY not configured', { status: 500 });
    }

    console.log(`[TTS] Requesting speech for: "${text.substring(0, 50)}..."`);

    // Request streaming TTS from ElevenLabs
    const response = await fetch(
      `https://api.elevenlabs.io/v1/text-to-speech/${VOICE_ID}/stream`,
      {
        method: 'POST',
        headers: {
          'xi-api-key': process.env.ELEVENLABS_API_KEY,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          text,
          model_id: MODEL_ID,
          output_format: 'mp3_44100_64', // Lower bitrate for faster streaming
          optimize_streaming_latency: 3, // Max latency optimization (0-4)
          voice_settings: {
            stability: 0.5,       // Balance consistency and expressiveness
            similarity_boost: 0.75,
            style: 0.0,          // Neutral style for guide voice
            use_speaker_boost: true,
          },
        }),
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      console.error('[TTS] ElevenLabs error:', response.status, errorText);
      return new Response(`TTS error: ${response.status}`, { status: response.status });
    }

    // Stream response directly to client
    return new Response(response.body, {
      headers: {
        'Content-Type': 'audio/mpeg',
        'Transfer-Encoding': 'chunked',
        'Cache-Control': 'no-cache',
      },
    });
  } catch (error) {
    console.error('[TTS] Error:', error);
    return new Response('TTS request failed', { status: 500 });
  }
}
```

Add ELEVENLABS_API_KEY to .env.local:
```
ELEVENLABS_API_KEY=your_key_here
```
  </action>
  <verify>
1. `npm run build` succeeds
2. Start dev server, POST to /api/jarvis/tts with `{"text": "test"}` returns audio stream or 500 if no key
  </verify>
  <done>TTS endpoint streams audio from ElevenLabs, API key server-side only</done>
</task>

<task type="auto">
  <name>Task 2: Create AudioPlayer with orb sync</name>
  <files>src/lib/jarvis/voice/AudioPlayer.ts</files>
  <action>
Create streaming audio player that:
1. Plays audio as it streams (not waiting for full download)
2. Analyzes audio output to drive orb's audioLevel
3. Reports playback state (playing, stopped)

```typescript
// src/lib/jarvis/voice/AudioPlayer.ts
import { useJarvisStore } from '../stores/jarvisStore';

export interface AudioPlayerCallbacks {
  onStart?: () => void;
  onEnd?: () => void;
  onError?: (error: Error) => void;
}

export class AudioPlayer {
  private audioContext: AudioContext | null = null;
  private analyser: AnalyserNode | null = null;
  private gainNode: GainNode | null = null;
  private dataArray: Float32Array | null = null;
  private isPlaying = false;
  private animationFrameId: number | null = null;
  private currentSource: AudioBufferSourceNode | null = null;
  private callbacks: AudioPlayerCallbacks;

  constructor(callbacks: AudioPlayerCallbacks = {}) {
    this.callbacks = callbacks;
  }

  /**
   * Initialize audio context (must be called after user interaction)
   */
  private async ensureContext(): Promise<void> {
    if (this.audioContext) {
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume();
      }
      return;
    }

    this.audioContext = new AudioContext();

    // Create analyser for orb reactivity
    this.analyser = this.audioContext.createAnalyser();
    this.analyser.fftSize = 256;
    this.dataArray = new Float32Array(this.analyser.fftSize);

    // Create gain node for volume control
    this.gainNode = this.audioContext.createGain();
    this.gainNode.gain.value = 1.0;

    // Connect: source -> analyser -> gain -> destination
    this.analyser.connect(this.gainNode);
    this.gainNode.connect(this.audioContext.destination);
  }

  /**
   * Play audio from a streaming response
   */
  async playStream(response: Response): Promise<void> {
    await this.ensureContext();

    if (!response.body) {
      throw new Error('Response has no body');
    }

    this.isPlaying = true;
    this.callbacks.onStart?.();
    useJarvisStore.getState().setOrbState('speaking');

    // Start audio level analysis loop
    this.startAnalysisLoop();

    try {
      // Collect all chunks (for MP3, we need full file to decode)
      // Note: True streaming playback requires WebCodecs or raw PCM
      // For Phase 2, we buffer MP3 and play when ready
      const chunks: Uint8Array[] = [];
      const reader = response.body.getReader();

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        chunks.push(value);
      }

      // Combine chunks
      const totalLength = chunks.reduce((acc, chunk) => acc + chunk.length, 0);
      const audioData = new Uint8Array(totalLength);
      let offset = 0;
      for (const chunk of chunks) {
        audioData.set(chunk, offset);
        offset += chunk.length;
      }

      // Decode and play
      const audioBuffer = await this.audioContext!.decodeAudioData(audioData.buffer);

      this.currentSource = this.audioContext!.createBufferSource();
      this.currentSource.buffer = audioBuffer;
      this.currentSource.connect(this.analyser!);

      this.currentSource.onended = () => {
        this.handlePlaybackEnd();
      };

      this.currentSource.start();
      console.log(`[AudioPlayer] Playing ${audioBuffer.duration.toFixed(2)}s of audio`);
    } catch (error) {
      console.error('[AudioPlayer] Playback error:', error);
      this.handlePlaybackEnd();
      this.callbacks.onError?.(error instanceof Error ? error : new Error(String(error)));
    }
  }

  /**
   * Stop playback immediately
   */
  stop(): void {
    if (this.currentSource) {
      try {
        this.currentSource.stop();
      } catch {
        // Already stopped
      }
      this.currentSource = null;
    }
    this.handlePlaybackEnd();
  }

  /**
   * Handle playback completion or stop
   */
  private handlePlaybackEnd(): void {
    this.isPlaying = false;
    this.stopAnalysisLoop();
    useJarvisStore.getState().setAudioLevel(0);
    this.callbacks.onEnd?.();
  }

  /**
   * Start audio level analysis loop for orb sync
   */
  private startAnalysisLoop(): void {
    if (this.animationFrameId !== null) return;

    const analyze = () => {
      if (!this.isPlaying || !this.analyser || !this.dataArray) {
        return;
      }

      this.analyser.getFloatTimeDomainData(this.dataArray);

      // Calculate RMS (same as MicrophoneCapture)
      let sum = 0;
      for (let i = 0; i < this.dataArray.length; i++) {
        sum += this.dataArray[i] * this.dataArray[i];
      }
      const rms = Math.sqrt(sum / this.dataArray.length);
      const normalizedLevel = Math.min(1, rms * 4);

      useJarvisStore.getState().setAudioLevel(normalizedLevel);

      this.animationFrameId = requestAnimationFrame(analyze);
    };

    analyze();
  }

  /**
   * Stop audio level analysis loop
   */
  private stopAnalysisLoop(): void {
    if (this.animationFrameId !== null) {
      cancelAnimationFrame(this.animationFrameId);
      this.animationFrameId = null;
    }
  }

  /**
   * Check if currently playing
   */
  getIsPlaying(): boolean {
    return this.isPlaying;
  }

  /**
   * Set volume (0-1)
   */
  setVolume(volume: number): void {
    if (this.gainNode) {
      this.gainNode.gain.value = Math.max(0, Math.min(1, volume));
    }
  }

  /**
   * Cleanup resources
   */
  cleanup(): void {
    this.stop();

    if (this.analyser) {
      this.analyser.disconnect();
      this.analyser = null;
    }

    if (this.gainNode) {
      this.gainNode.disconnect();
      this.gainNode = null;
    }

    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    this.dataArray = null;
  }
}
```
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
2. AudioPlayer exports correctly
  </verify>
  <done>AudioPlayer plays streaming audio and drives orb audioLevel during playback</done>
</task>

<task type="auto">
  <name>Task 3: Create ElevenLabsClient</name>
  <files>src/lib/jarvis/voice/ElevenLabsClient.ts, src/lib/jarvis/voice/types.ts, src/lib/jarvis/voice/index.ts</files>
  <action>
Create browser-side TTS client that requests speech and plays via AudioPlayer.

First, add TTS types to src/lib/jarvis/voice/types.ts:

```typescript
// Add to existing types.ts

// TTS configuration
export interface TTSConfig {
  voiceId?: string;       // Default: Stuart
  modelId?: string;       // Default: eleven_flash_v2_5
  stability?: number;     // 0-1, default: 0.5
  similarityBoost?: number; // 0-1, default: 0.75
}

// TTS callbacks
export interface TTSCallbacks {
  onStart?: () => void;
  onEnd?: () => void;
  onError?: (error: Error) => void;
}
```

Create ElevenLabsClient in src/lib/jarvis/voice/ElevenLabsClient.ts:

```typescript
// src/lib/jarvis/voice/ElevenLabsClient.ts
import { TTSCallbacks, TTSConfig } from './types';
import { AudioPlayer } from './AudioPlayer';

export class ElevenLabsClient {
  private audioPlayer: AudioPlayer;
  private callbacks: TTSCallbacks;
  private config: TTSConfig;
  private abortController: AbortController | null = null;

  constructor(callbacks: TTSCallbacks = {}, config: TTSConfig = {}) {
    this.callbacks = callbacks;
    this.config = config;

    this.audioPlayer = new AudioPlayer({
      onStart: () => {
        console.log('[ElevenLabsClient] Playback started');
        this.callbacks.onStart?.();
      },
      onEnd: () => {
        console.log('[ElevenLabsClient] Playback ended');
        this.callbacks.onEnd?.();
      },
      onError: (error) => {
        console.error('[ElevenLabsClient] Playback error:', error);
        this.callbacks.onError?.(error);
      },
    });
  }

  /**
   * Speak the given text
   */
  async speak(text: string): Promise<void> {
    if (!text.trim()) {
      console.warn('[ElevenLabsClient] Empty text, skipping');
      return;
    }

    // Cancel any in-progress request
    this.stop();

    this.abortController = new AbortController();

    try {
      console.log(`[ElevenLabsClient] Requesting speech: "${text.substring(0, 50)}..."`);
      const startTime = performance.now();

      const response = await fetch('/api/jarvis/tts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ text }),
        signal: this.abortController.signal,
      });

      if (!response.ok) {
        throw new Error(`TTS request failed: ${response.status}`);
      }

      const latency = performance.now() - startTime;
      console.log(`[ElevenLabsClient] First byte in ${latency.toFixed(1)}ms`);

      await this.audioPlayer.playStream(response);
    } catch (error) {
      if (error instanceof Error && error.name === 'AbortError') {
        console.log('[ElevenLabsClient] Request aborted');
        return;
      }
      console.error('[ElevenLabsClient] Error:', error);
      this.callbacks.onError?.(error instanceof Error ? error : new Error(String(error)));
    }
  }

  /**
   * Stop current speech
   */
  stop(): void {
    if (this.abortController) {
      this.abortController.abort();
      this.abortController = null;
    }
    this.audioPlayer.stop();
  }

  /**
   * Check if currently speaking
   */
  getIsSpeaking(): boolean {
    return this.audioPlayer.getIsPlaying();
  }

  /**
   * Set volume
   */
  setVolume(volume: number): void {
    this.audioPlayer.setVolume(volume);
  }

  /**
   * Cleanup resources
   */
  cleanup(): void {
    this.stop();
    this.audioPlayer.cleanup();
  }
}
```

Update barrel export in src/lib/jarvis/voice/index.ts:
```typescript
export * from './types';
export { DeepgramClient } from './DeepgramClient';
export { AudioPlayer } from './AudioPlayer';
export { ElevenLabsClient } from './ElevenLabsClient';
```
  </action>
  <verify>
1. TypeScript compiles: `npx tsc --noEmit`
2. All exports present in index.ts
  </verify>
  <done>ElevenLabsClient requests TTS and plays audio with orb sync</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Build check:**
   ```bash
   npm run build
   ```

2. **Manual verification (requires API key):**
   - Add ELEVENLABS_API_KEY to .env.local
   - Start dev server
   - In browser console:
     ```javascript
     const client = new ElevenLabsClient({ onStart: () => console.log('speaking'), onEnd: () => console.log('done') });
     client.speak("Hello, I am Jarvis.");
     ```
   - Verify audio plays and orb animates

3. **Latency measurement:**
   - Check console logs for "First byte in Xms"
   - Target: < 150ms to first byte

4. **API key security:**
   - Network tab shows POST to /api/jarvis/tts, not api.elevenlabs.io
   - No API key in request headers
</verification>

<success_criteria>
- ElevenLabsClient.speak(text) plays audio through browser
- Orb's audioLevel updates during playback (visible animation)
- ElevenLabsClient.stop() cancels request and stops playback
- First audio byte latency logged to console
- API key visible only in server logs
- No TypeScript errors in build
</success_criteria>

<output>
After completion, create `jarvis/.planning/phases/02-voice-pipeline/02-02-SUMMARY.md`
</output>
