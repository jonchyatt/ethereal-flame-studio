---
phase: 01-audio-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/lib/jarvis/audio/MicrophoneCapture.ts
  - src/lib/jarvis/audio/VoiceActivityDetector.ts
  - src/components/jarvis/PushToTalk.tsx
  - src/components/jarvis/PermissionPrompt.tsx
  - src/app/jarvis/page.tsx
autonomous: true

must_haves:
  truths:
    - "User sees explanatory UI BEFORE microphone permission prompt"
    - "User can press and hold button to capture audio"
    - "Orb responds to voice amplitude while button held"
    - "Permission denial shows recovery instructions (not broken state)"
    - "Audio latency instrumentation captures timing data"
  artifacts:
    - path: "src/lib/jarvis/audio/MicrophoneCapture.ts"
      provides: "MediaStream microphone capture with amplitude analysis"
      exports: ["MicrophoneCapture"]
    - path: "src/lib/jarvis/audio/VoiceActivityDetector.ts"
      provides: "Simple VAD using amplitude threshold"
      exports: ["VoiceActivityDetector"]
    - path: "src/components/jarvis/PushToTalk.tsx"
      provides: "Push-to-talk button with press/release handling"
      exports: ["PushToTalk"]
    - path: "src/components/jarvis/PermissionPrompt.tsx"
      provides: "First-time permission explanation UI"
      exports: ["PermissionPrompt"]
  key_links:
    - from: "src/components/jarvis/PushToTalk.tsx"
      to: "src/lib/jarvis/audio/MicrophoneCapture.ts"
      via: "MicrophoneCapture.start() on button press"
      pattern: "MicrophoneCapture"
    - from: "src/lib/jarvis/audio/MicrophoneCapture.ts"
      to: "src/lib/jarvis/stores/jarvisStore.ts"
      via: "setAudioLevel() updates"
      pattern: "setAudioLevel"
    - from: "src/app/jarvis/page.tsx"
      to: "src/components/jarvis/PushToTalk.tsx"
      via: "PushToTalk component"
      pattern: "<PushToTalk"
---

<objective>
Implement browser audio capture with push-to-talk interaction and permission UX.

Purpose: Enable users to speak to Jarvis through their microphone. The permission flow is critical - users must understand WHY microphone access is needed BEFORE the browser prompt appears. Push-to-talk (hold to speak) is more reliable than always-on listening.

Output: Working push-to-talk interface that captures audio, analyzes amplitude, and updates the orb visualization in real-time.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Existing audio analysis for reference
@src/lib/audio/AudioAnalyzer.ts

# Jarvis store from Plan 01
@src/lib/jarvis/types.ts
@src/lib/jarvis/stores/jarvisStore.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MicrophoneCapture class</name>
  <files>src/lib/jarvis/audio/MicrophoneCapture.ts</files>
  <action>
Create a class for microphone capture with real-time amplitude analysis:

```typescript
import { useJarvisStore } from '../stores/jarvisStore';

export interface AudioCaptureOptions {
  sampleRate?: number;      // Default 16000 (good for speech)
  fftSize?: number;         // Default 256 (fast analysis)
  onAmplitude?: (level: number) => void;
  onError?: (error: Error) => void;
}

export class MicrophoneCapture {
  private audioContext: AudioContext | null = null;
  private analyser: AnalyserNode | null = null;
  private source: MediaStreamAudioSourceNode | null = null;
  private stream: MediaStream | null = null;
  private dataArray: Uint8Array | null = null;
  private animationId: number | null = null;

  private options: Required<AudioCaptureOptions>;

  // Latency instrumentation
  private captureStartTime: number = 0;
  public lastLatency: number = 0;

  constructor(options: AudioCaptureOptions = {}) {
    this.options = {
      sampleRate: options.sampleRate ?? 16000,
      fftSize: options.fftSize ?? 256,
      onAmplitude: options.onAmplitude ?? (() => {}),
      onError: options.onError ?? console.error,
    };
  }

  /**
   * Request microphone permission without starting capture
   * Call this in response to user action to show permission prompt
   */
  async requestPermission(): Promise<boolean> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: this.options.sampleRate,
        }
      });

      // Got permission - stop stream immediately
      stream.getTracks().forEach(track => track.stop());

      // Update store
      useJarvisStore.getState().setAudioPermissionGranted(true);
      return true;
    } catch (error) {
      console.error('Microphone permission denied:', error);
      useJarvisStore.getState().setAudioPermissionGranted(false);
      return false;
    }
  }

  /**
   * Start capturing audio from microphone
   * Should only be called after permission granted
   */
  async start(): Promise<void> {
    // Record start time for latency measurement
    this.captureStartTime = performance.now();

    try {
      this.stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: this.options.sampleRate,
        }
      });

      // Create audio context
      this.audioContext = new AudioContext({ sampleRate: this.options.sampleRate });

      // Create analyser for amplitude measurement
      this.analyser = this.audioContext.createAnalyser();
      this.analyser.fftSize = this.options.fftSize;
      this.analyser.smoothingTimeConstant = 0.3; // Fast response

      // Connect microphone to analyser
      this.source = this.audioContext.createMediaStreamSource(this.stream);
      this.source.connect(this.analyser);

      // Allocate data array
      this.dataArray = new Uint8Array(this.analyser.frequencyBinCount);

      // Calculate latency
      this.lastLatency = performance.now() - this.captureStartTime;
      console.log(`Audio capture started, latency: ${this.lastLatency.toFixed(1)}ms`);

      // Update store
      useJarvisStore.getState().setIsCapturing(true);
      useJarvisStore.getState().setAudioPermissionGranted(true);

      // Start amplitude analysis loop
      this.analyzeAmplitude();

    } catch (error) {
      this.options.onError(error as Error);
      this.stop();
    }
  }

  /**
   * Stop capturing and release resources
   */
  stop(): void {
    if (this.animationId) {
      cancelAnimationFrame(this.animationId);
      this.animationId = null;
    }

    if (this.source) {
      this.source.disconnect();
      this.source = null;
    }

    if (this.analyser) {
      this.analyser.disconnect();
      this.analyser = null;
    }

    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    if (this.stream) {
      this.stream.getTracks().forEach(track => track.stop());
      this.stream = null;
    }

    this.dataArray = null;

    // Update store
    useJarvisStore.getState().setIsCapturing(false);
    useJarvisStore.getState().setAudioLevel(0);
  }

  /**
   * Continuous amplitude analysis loop
   */
  private analyzeAmplitude = (): void => {
    if (!this.analyser || !this.dataArray) return;

    // Get frequency data
    this.analyser.getByteFrequencyData(this.dataArray);

    // Calculate RMS amplitude (more accurate than peak)
    let sum = 0;
    for (let i = 0; i < this.dataArray.length; i++) {
      const normalized = this.dataArray[i] / 255;
      sum += normalized * normalized;
    }
    const rms = Math.sqrt(sum / this.dataArray.length);

    // Apply slight boost for better visual response
    const level = Math.min(1, rms * 2);

    // Update store and callback
    useJarvisStore.getState().setAudioLevel(level);
    this.options.onAmplitude(level);

    // Continue loop
    this.animationId = requestAnimationFrame(this.analyzeAmplitude);
  };

  /**
   * Check if microphone permission was previously granted
   */
  static async checkPermission(): Promise<PermissionState> {
    try {
      const result = await navigator.permissions.query({
        name: 'microphone' as PermissionName
      });
      return result.state;
    } catch {
      // Permissions API not supported - assume prompt
      return 'prompt';
    }
  }
}

// Singleton instance for app-wide use
let instance: MicrophoneCapture | null = null;

export function getMicrophoneCapture(options?: AudioCaptureOptions): MicrophoneCapture {
  if (!instance) {
    instance = new MicrophoneCapture(options);
  }
  return instance;
}
```

Key features:
- RMS amplitude calculation (more accurate than peak)
- Latency instrumentation (logs capture start time)
- Singleton pattern for app-wide access
- Store integration (setAudioLevel, setIsCapturing)
- Clean resource cleanup on stop()
  </action>
  <verify>TypeScript compiles without errors</verify>
  <done>MicrophoneCapture class with amplitude analysis and latency instrumentation</done>
</task>

<task type="auto">
  <name>Task 2: Create permission prompt component</name>
  <files>src/components/jarvis/PermissionPrompt.tsx</files>
  <action>
Create an explanatory UI that appears BEFORE the browser permission prompt:

```typescript
'use client';

import { useState } from 'react';
import { getMicrophoneCapture, MicrophoneCapture } from '@/lib/jarvis/audio/MicrophoneCapture';

interface PermissionPromptProps {
  onPermissionGranted: () => void;
  onPermissionDenied: () => void;
}

export function PermissionPrompt({ onPermissionGranted, onPermissionDenied }: PermissionPromptProps) {
  const [isRequesting, setIsRequesting] = useState(false);
  const [isDenied, setIsDenied] = useState(false);

  const handleEnable = async () => {
    setIsRequesting(true);
    setIsDenied(false);

    const capture = getMicrophoneCapture();
    const granted = await capture.requestPermission();

    setIsRequesting(false);

    if (granted) {
      onPermissionGranted();
    } else {
      setIsDenied(true);
      onPermissionDenied();
    }
  };

  if (isDenied) {
    return (
      <div className="flex flex-col items-center text-center px-6 py-8 max-w-sm mx-auto">
        <div className="w-16 h-16 rounded-full bg-red-500/20 flex items-center justify-center mb-4">
          <svg className="w-8 h-8 text-red-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M6 18L18 6M6 6l12 12" />
          </svg>
        </div>

        <h2 className="text-xl font-semibold text-white mb-2">
          Microphone Access Denied
        </h2>

        <p className="text-white/60 text-sm mb-6">
          Jarvis needs microphone access to hear you. Your browser remembered this choice.
        </p>

        <div className="bg-white/5 rounded-lg p-4 text-left text-sm text-white/70 space-y-2">
          <p className="font-medium text-white/90">To enable microphone:</p>
          <ol className="list-decimal list-inside space-y-1">
            <li>Click the lock/info icon in your browser address bar</li>
            <li>Find &quot;Microphone&quot; permission</li>
            <li>Change from &quot;Block&quot; to &quot;Allow&quot;</li>
            <li>Refresh this page</li>
          </ol>
        </div>

        <button
          onClick={() => window.location.reload()}
          className="mt-6 px-6 py-3 bg-white/10 hover:bg-white/20 text-white rounded-full transition-colors"
        >
          Refresh Page
        </button>
      </div>
    );
  }

  return (
    <div className="flex flex-col items-center text-center px-6 py-8 max-w-sm mx-auto">
      <div className="w-16 h-16 rounded-full bg-cyan-500/20 flex items-center justify-center mb-4">
        <svg className="w-8 h-8 text-cyan-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
          <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
        </svg>
      </div>

      <h2 className="text-xl font-semibold text-white mb-2">
        Enable Voice
      </h2>

      <p className="text-white/60 text-sm mb-6">
        Jarvis needs microphone access to hear you when you hold the talk button.
        Your voice is processed locally and never stored.
      </p>

      <button
        onClick={handleEnable}
        disabled={isRequesting}
        className={`
          px-8 py-3 rounded-full font-medium text-base
          transition-all
          ${isRequesting
            ? 'bg-white/20 text-white/50 cursor-wait'
            : 'bg-cyan-500 hover:bg-cyan-400 text-white shadow-lg shadow-cyan-500/30'
          }
        `}
      >
        {isRequesting ? 'Requesting...' : 'Enable Microphone'}
      </button>

      <p className="text-white/40 text-xs mt-4">
        You can revoke access anytime in browser settings
      </p>
    </div>
  );
}
```

Key features:
- Clear explanation of WHY microphone is needed
- Privacy assurance (processed locally, not stored)
- Denial recovery instructions (how to re-enable)
- Refresh button for denied state
- No dark patterns - honest, clear messaging
  </action>
  <verify>Component renders and clicking "Enable" triggers browser permission prompt</verify>
  <done>PermissionPrompt with explanatory UI and denial recovery instructions</done>
</task>

<task type="auto">
  <name>Task 3: Create PushToTalk button component</name>
  <files>src/components/jarvis/PushToTalk.tsx</files>
  <action>
Create the push-to-talk button with press/release handling:

```typescript
'use client';

import { useRef, useCallback, useEffect } from 'react';
import { getMicrophoneCapture } from '@/lib/jarvis/audio/MicrophoneCapture';
import { useJarvisStore } from '@/lib/jarvis/stores/jarvisStore';

export function PushToTalk() {
  const isCapturing = useJarvisStore((s) => s.isCapturing);
  const setOrbState = useJarvisStore((s) => s.setOrbState);
  const audioLevel = useJarvisStore((s) => s.audioLevel);

  const captureRef = useRef(getMicrophoneCapture());
  const isPressing = useRef(false);

  const handlePressStart = useCallback(async () => {
    if (isPressing.current) return;
    isPressing.current = true;

    // Start capture
    await captureRef.current.start();

    // Set orb to listening state
    setOrbState('listening');
  }, [setOrbState]);

  const handlePressEnd = useCallback(() => {
    if (!isPressing.current) return;
    isPressing.current = false;

    // Stop capture
    captureRef.current.stop();

    // Return to idle state
    setOrbState('idle');
  }, [setOrbState]);

  // Mouse events
  const handleMouseDown = useCallback((e: React.MouseEvent) => {
    e.preventDefault();
    handlePressStart();
  }, [handlePressStart]);

  const handleMouseUp = useCallback(() => {
    handlePressEnd();
  }, [handlePressEnd]);

  // Touch events
  const handleTouchStart = useCallback((e: React.TouchEvent) => {
    e.preventDefault();
    handlePressStart();
  }, [handlePressStart]);

  const handleTouchEnd = useCallback(() => {
    handlePressEnd();
  }, [handlePressEnd]);

  // Keyboard support (spacebar)
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (e.code === 'Space' && !e.repeat) {
        e.preventDefault();
        handlePressStart();
      }
    };

    const handleKeyUp = (e: KeyboardEvent) => {
      if (e.code === 'Space') {
        e.preventDefault();
        handlePressEnd();
      }
    };

    window.addEventListener('keydown', handleKeyDown);
    window.addEventListener('keyup', handleKeyUp);

    return () => {
      window.removeEventListener('keydown', handleKeyDown);
      window.removeEventListener('keyup', handleKeyUp);
    };
  }, [handlePressStart, handlePressEnd]);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      captureRef.current.stop();
    };
  }, []);

  // Visual feedback based on audio level
  const pulseScale = isCapturing ? 1 + audioLevel * 0.2 : 1;
  const glowIntensity = isCapturing ? 0.5 + audioLevel * 0.5 : 0.3;

  return (
    <div className="flex flex-col items-center">
      {/* Push-to-talk button */}
      <button
        onMouseDown={handleMouseDown}
        onMouseUp={handleMouseUp}
        onMouseLeave={handleMouseUp}
        onTouchStart={handleTouchStart}
        onTouchEnd={handleTouchEnd}
        className={`
          relative w-20 h-20 rounded-full
          flex items-center justify-center
          transition-all duration-150
          touch-none select-none
          ${isCapturing
            ? 'bg-cyan-500 scale-95'
            : 'bg-white/10 hover:bg-white/20 active:scale-95'
          }
        `}
        style={{
          transform: `scale(${pulseScale})`,
          boxShadow: isCapturing
            ? `0 0 ${40 * glowIntensity}px ${20 * glowIntensity}px rgba(6, 182, 212, ${glowIntensity})`
            : 'none',
        }}
        aria-label="Hold to talk"
      >
        {/* Microphone icon */}
        <svg
          className={`w-8 h-8 ${isCapturing ? 'text-white' : 'text-white/70'}`}
          fill="none"
          viewBox="0 0 24 24"
          stroke="currentColor"
        >
          <path
            strokeLinecap="round"
            strokeLinejoin="round"
            strokeWidth={2}
            d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"
          />
        </svg>
      </button>

      {/* Instruction text */}
      <p className="text-white/50 text-sm mt-4">
        {isCapturing ? 'Listening...' : 'Hold to talk'}
      </p>

      {/* Keyboard hint */}
      <p className="text-white/30 text-xs mt-1">
        or hold <kbd className="px-1.5 py-0.5 bg-white/10 rounded text-white/50">Space</kbd>
      </p>
    </div>
  );
}
```

Key features:
- Press-and-hold interaction (mouse, touch, keyboard)
- Visual feedback (glow, scale) responding to audio level
- Spacebar support for desktop users
- Touch-action: none to prevent scroll on mobile
- Clean release handling (mouse leave, touch end)
  </action>
  <verify>Button responds to press/release, orb state changes, audio level updates</verify>
  <done>PushToTalk button with mouse, touch, and keyboard support</done>
</task>

<task type="auto">
  <name>Task 4: Integrate audio capture into Jarvis page</name>
  <files>src/app/jarvis/page.tsx</files>
  <action>
Update Jarvis page to handle permission flow and show push-to-talk:

```typescript
'use client';

import { useEffect, useState } from 'react';
import { JarvisOrb } from '@/components/jarvis/JarvisOrb';
import { PushToTalk } from '@/components/jarvis/PushToTalk';
import { PermissionPrompt } from '@/components/jarvis/PermissionPrompt';
import { MicrophoneCapture } from '@/lib/jarvis/audio/MicrophoneCapture';
import { useJarvisStore } from '@/lib/jarvis/stores/jarvisStore';

type PermissionState = 'checking' | 'prompt' | 'granted' | 'denied';

export default function JarvisPage() {
  const [permissionState, setPermissionState] = useState<PermissionState>('checking');
  const isAudioPermissionGranted = useJarvisStore((s) => s.isAudioPermissionGranted);

  // Check permission status on mount
  useEffect(() => {
    const checkPermission = async () => {
      const state = await MicrophoneCapture.checkPermission();

      if (state === 'granted') {
        useJarvisStore.getState().setAudioPermissionGranted(true);
        setPermissionState('granted');
      } else if (state === 'denied') {
        setPermissionState('denied');
      } else {
        setPermissionState('prompt');
      }
    };

    checkPermission();
  }, []);

  // Sync store state with local state
  useEffect(() => {
    if (isAudioPermissionGranted) {
      setPermissionState('granted');
    }
  }, [isAudioPermissionGranted]);

  const handlePermissionGranted = () => {
    setPermissionState('granted');
  };

  const handlePermissionDenied = () => {
    setPermissionState('denied');
  };

  // Loading state
  if (permissionState === 'checking') {
    return (
      <main className="flex flex-col h-full w-full items-center justify-center">
        <div className="w-8 h-8 border-2 border-white/30 border-t-white rounded-full animate-spin" />
      </main>
    );
  }

  // Permission prompt state
  if (permissionState === 'prompt' || permissionState === 'denied') {
    return (
      <main className="flex flex-col h-full w-full">
        {/* Still show orb in background */}
        <div className="flex-1 relative opacity-30">
          <JarvisOrb />
        </div>

        {/* Permission prompt overlay */}
        <div className="absolute inset-0 flex items-center justify-center bg-black/50 backdrop-blur-sm">
          <PermissionPrompt
            onPermissionGranted={handlePermissionGranted}
            onPermissionDenied={handlePermissionDenied}
          />
        </div>
      </main>
    );
  }

  // Main experience (permission granted)
  return (
    <main className="flex flex-col h-full w-full">
      {/* Orb container - takes majority of screen */}
      <div className="flex-1 relative">
        <JarvisOrb />
      </div>

      {/* Push-to-talk controls */}
      <div className="pb-8 px-4">
        <PushToTalk />
      </div>
    </main>
  );
}
```

This creates the full permission flow:
1. Check existing permission on mount
2. If granted, show push-to-talk immediately
3. If prompt/denied, show PermissionPrompt
4. Orb visible (dimmed) behind permission prompt
5. After permission granted, show push-to-talk
  </action>
  <verify>
1. First visit shows permission prompt
2. Clicking "Enable" shows browser permission dialog
3. After granting, push-to-talk button appears
4. Holding button captures audio and orb responds
5. Denying permission shows recovery instructions
  </verify>
  <done>Complete permission flow integrated, push-to-talk working with orb response</done>
</task>

</tasks>

<verification>
1. `npm run dev` starts without errors
2. Navigate to http://localhost:3000/jarvis (fresh browser / incognito)
3. See permission prompt with explanation
4. Click "Enable Microphone" - browser permission dialog appears
5. Grant permission - push-to-talk button appears
6. Hold push-to-talk button - orb turns cyan (listening state)
7. Speak into microphone - orb pulses in response to voice
8. Release button - orb returns to blue (idle state)
9. Test keyboard: hold spacebar - same behavior
10. Test on mobile: touch and hold button
11. Test denial: In browser settings, block microphone, refresh
12. See denial recovery instructions with steps to re-enable
</verification>

<success_criteria>
- Explanatory UI appears BEFORE browser permission prompt
- Push-to-talk button captures audio while held
- Orb responds to voice amplitude in real-time
- Permission denial shows clear recovery instructions
- Works on desktop (mouse, keyboard) and mobile (touch)
- Audio latency logged to console on capture start
</success_criteria>

<output>
After completion, create `.planning/phases/01-audio-foundation/01-03-SUMMARY.md`
</output>
