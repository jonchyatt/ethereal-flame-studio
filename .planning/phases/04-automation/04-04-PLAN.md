---
phase: 04-automation
plan: 04
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - whisper-service/main.py
  - whisper-service/requirements.txt
  - whisper-service/Dockerfile
  - docker-compose.yml
autonomous: true
user_setup:
  - service: faster-whisper
    why: "GPU-accelerated transcription for video descriptions"
    env_vars:
      - name: WHISPER_MODEL
        source: "Default: large-v3 (4.5GB VRAM) or medium (1.5GB)"
    dashboard_config:
      - task: "Ensure NVIDIA drivers and CUDA installed"
        location: "nvidia-smi should show GPU"

must_haves:
  truths:
    - "Audio files can be transcribed to text descriptions"
    - "Transcription service runs independently via Docker"
    - "Transcription results stored in SQLite metadata"
  artifacts:
    - path: "whisper-service/main.py"
      provides: "FastAPI service for Whisper transcription"
      contains: "WhisperModel"
    - path: "whisper-service/Dockerfile"
      provides: "GPU-enabled container for transcription"
      contains: "nvidia/cuda"
    - path: "docker-compose.yml"
      provides: "whisper service definition"
      contains: "whisper:"
  key_links:
    - from: "whisper-service/main.py"
      to: "faster_whisper"
      via: "transcription library"
      pattern: "from faster_whisper import"
    - from: "Node.js worker"
      to: "whisper-service"
      via: "HTTP API"
      pattern: "fetch.*transcribe"
---

<objective>
Create a faster-whisper microservice for automatic video description generation.

Purpose: Automatically transcribe audio content to generate video descriptions for YouTube and social media. Runs as a separate Python service to avoid loading the large Whisper model in the Node.js process.

Output: Dockerized FastAPI service with `/transcribe` endpoint, integrated into docker-compose stack.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-automation/04-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create faster-whisper FastAPI service</name>
  <files>whisper-service/main.py, whisper-service/requirements.txt</files>
  <action>
Create `whisper-service/requirements.txt`:
```
fastapi==0.115.0
uvicorn[standard]==0.32.0
faster-whisper==1.1.0
python-multipart==0.0.17
pydantic==2.10.0
```

Create `whisper-service/main.py`:
```python
import os
import tempfile
from pathlib import Path
from typing import Optional

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from faster_whisper import WhisperModel

app = FastAPI(title="Ethereal Flame Whisper Service")

# Model configuration
MODEL_SIZE = os.environ.get("WHISPER_MODEL", "large-v3")
DEVICE = os.environ.get("WHISPER_DEVICE", "cuda")  # cuda or cpu
COMPUTE_TYPE = os.environ.get("WHISPER_COMPUTE_TYPE", "float16")  # float16, int8, or float32

# Lazy model loading
_model: Optional[WhisperModel] = None


def get_model() -> WhisperModel:
    global _model
    if _model is None:
        print(f"Loading Whisper model: {MODEL_SIZE} on {DEVICE} ({COMPUTE_TYPE})...")
        _model = WhisperModel(
            MODEL_SIZE,
            device=DEVICE,
            compute_type=COMPUTE_TYPE
        )
        print("Model loaded successfully")
    return _model


class TranscriptionResult(BaseModel):
    text: str
    language: str
    duration_seconds: float
    segments_count: int


class TranscriptionRequest(BaseModel):
    audio_path: str
    language: Optional[str] = None  # Auto-detect if not specified


@app.get("/health")
async def health():
    """Health check endpoint"""
    return {"status": "healthy", "model": MODEL_SIZE, "device": DEVICE}


@app.post("/transcribe", response_model=TranscriptionResult)
async def transcribe_file(file: UploadFile = File(...)):
    """
    Transcribe uploaded audio file and return text.
    Accepts: MP3, WAV, OGG, FLAC, M4A
    """
    if not file.filename:
        raise HTTPException(400, "No filename provided")

    # Save to temp file (Whisper needs file path)
    suffix = Path(file.filename).suffix
    with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = tmp.name

    try:
        model = get_model()

        # Transcribe with beam search for better accuracy
        segments, info = model.transcribe(
            tmp_path,
            beam_size=5,
            word_timestamps=False,  # Not needed for descriptions
            vad_filter=True,  # Filter out silence
        )

        # Collect all segments
        all_segments = list(segments)
        full_text = " ".join([seg.text.strip() for seg in all_segments])

        return TranscriptionResult(
            text=full_text,
            language=info.language,
            duration_seconds=info.duration,
            segments_count=len(all_segments)
        )

    except Exception as e:
        raise HTTPException(500, f"Transcription failed: {str(e)}")

    finally:
        # Clean up temp file
        os.unlink(tmp_path)


@app.post("/transcribe-path", response_model=TranscriptionResult)
async def transcribe_path(request: TranscriptionRequest):
    """
    Transcribe audio file from local path (for internal use).
    Useful when audio is already on shared volume.
    """
    if not os.path.exists(request.audio_path):
        raise HTTPException(404, f"File not found: {request.audio_path}")

    try:
        model = get_model()

        segments, info = model.transcribe(
            request.audio_path,
            beam_size=5,
            language=request.language,
            vad_filter=True,
        )

        all_segments = list(segments)
        full_text = " ".join([seg.text.strip() for seg in all_segments])

        return TranscriptionResult(
            text=full_text,
            language=info.language,
            duration_seconds=info.duration,
            segments_count=len(all_segments)
        )

    except Exception as e:
        raise HTTPException(500, f"Transcription failed: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
```

Key features:
- Lazy model loading (only loads on first request)
- Two endpoints: file upload and local path
- VAD filter removes silence for cleaner transcription
- Returns language detection and duration
  </action>
  <verify>
Test locally (requires Python environment):
```bash
cd whisper-service
pip install -r requirements.txt
python main.py
# In another terminal:
curl -X POST http://localhost:8001/health
# {"status":"healthy","model":"large-v3","device":"cuda"}

# Test with a small audio file
curl -X POST -F "file=@test.mp3" http://localhost:8001/transcribe
```
  </verify>
  <done>
FastAPI service runs, loads Whisper model on first request, transcribes audio to text.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Docker configuration for Whisper service</name>
  <files>whisper-service/Dockerfile, docker-compose.yml</files>
  <action>
Create `whisper-service/Dockerfile`:
```dockerfile
FROM nvidia/cuda:12.1-runtime-ubuntu22.04

# Install Python
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3-pip \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Set up working directory
WORKDIR /app

# Install Python dependencies
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Copy application
COPY main.py .

# Pre-download model (optional, speeds up first request)
# Uncomment to include model in image (adds ~3GB to image size)
# RUN python3 -c "from faster_whisper import WhisperModel; WhisperModel('large-v3', device='cpu')"

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8001/health || exit 1

EXPOSE 8001

CMD ["python3", "main.py"]
```

Update `docker-compose.yml` to add whisper service:
```yaml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    container_name: ethereal-redis
    command: redis-server --maxmemory-policy noeviction --appendonly yes
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  whisper:
    build:
      context: ./whisper-service
      dockerfile: Dockerfile
    container_name: ethereal-whisper
    ports:
      - "8001:8001"
    volumes:
      - ./data/audio:/audio:ro  # Read-only access to audio files
      - whisper-cache:/root/.cache/huggingface  # Cache model downloads
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-large-v3}
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s  # Model loading can take time
      retries: 3

volumes:
  redis-data:
  whisper-cache:
```

Add to `.env.example`:
```
WHISPER_MODEL=large-v3
WHISPER_SERVICE_URL=http://localhost:8001
```
  </action>
  <verify>
```bash
# Build and start
docker-compose build whisper
docker-compose up -d whisper

# Check logs (model download on first run)
docker-compose logs -f whisper

# Test health
curl http://localhost:8001/health

# Test transcription
curl -X POST -F "file=@test-audio.mp3" http://localhost:8001/transcribe
```
  </verify>
  <done>
Whisper service runs in Docker with GPU support, model cached in volume, health checks passing.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create Node.js client for Whisper service</name>
  <files>src/lib/services/whisperClient.ts</files>
  <action>
Create `src/lib/services/whisperClient.ts`:

```typescript
import { createReadStream } from 'fs';
import { stat } from 'fs/promises';
import FormData from 'form-data';

const WHISPER_URL = process.env.WHISPER_SERVICE_URL || 'http://localhost:8001';

export interface TranscriptionResult {
  text: string;
  language: string;
  durationSeconds: number;
  segmentsCount: number;
}

export async function checkWhisperHealth(): Promise<boolean> {
  try {
    const response = await fetch(`${WHISPER_URL}/health`);
    return response.ok;
  } catch {
    return false;
  }
}

export async function transcribeFile(filePath: string): Promise<TranscriptionResult> {
  // Verify file exists
  await stat(filePath);

  // Create form data with file
  const form = new FormData();
  form.append('file', createReadStream(filePath));

  const response = await fetch(`${WHISPER_URL}/transcribe`, {
    method: 'POST',
    body: form as unknown as BodyInit,
    headers: form.getHeaders(),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Transcription failed: ${response.status} - ${error}`);
  }

  const result = await response.json();

  return {
    text: result.text,
    language: result.language,
    durationSeconds: result.duration_seconds,
    segmentsCount: result.segments_count,
  };
}

export async function transcribePath(audioPath: string): Promise<TranscriptionResult> {
  const response = await fetch(`${WHISPER_URL}/transcribe-path`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ audio_path: audioPath }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Transcription failed: ${response.status} - ${error}`);
  }

  const result = await response.json();

  return {
    text: result.text,
    language: result.language,
    durationSeconds: result.duration_seconds,
    segmentsCount: result.segments_count,
  };
}

// Generate a YouTube-friendly description from transcription
export function formatVideoDescription(
  transcription: TranscriptionResult,
  audioName: string
): string {
  const lines = [
    `${audioName}`,
    '',
    transcription.text.slice(0, 500) + (transcription.text.length > 500 ? '...' : ''),
    '',
    `Duration: ${Math.round(transcription.durationSeconds / 60)} minutes`,
    `Language: ${transcription.language}`,
    '',
    '---',
    'Created with Ethereal Flame Studio',
  ];

  return lines.join('\n');
}
```

Install form-data for file uploads:
```bash
npm install form-data
npm install -D @types/form-data
```
  </action>
  <verify>
Test script:
```typescript
import { transcribeFile, formatVideoDescription, checkWhisperHealth } from './whisperClient';

async function test() {
  const healthy = await checkWhisperHealth();
  console.log('Whisper healthy:', healthy);

  if (healthy) {
    const result = await transcribeFile('./test-audio.mp3');
    console.log('Transcription:', result.text.slice(0, 100) + '...');
    console.log('Duration:', result.durationSeconds);

    const description = formatVideoDescription(result, 'Morning Meditation');
    console.log('\nFormatted description:\n', description);
  }
}

test().catch(console.error);
```
  </verify>
  <done>
Node.js client can call Whisper service, format results as video descriptions.
  </done>
</task>

</tasks>

<verification>
1. `docker-compose up -d whisper` starts the service
2. `curl http://localhost:8001/health` returns healthy status
3. File upload transcription works via curl
4. Node.js client can transcribe files
5. GPU utilization visible during transcription (nvidia-smi)
6. Model cached in Docker volume (subsequent starts are fast)
</verification>

<success_criteria>
- Whisper service runs in Docker with GPU acceleration
- Model loads on first request, cached for subsequent requests
- Both file upload and local path transcription work
- Node.js client provides clean interface
- Video descriptions formatted for YouTube/social
- Service health checks pass
</success_criteria>

<output>
After completion, create `.planning/phases/04-automation/04-04-SUMMARY.md`
</output>
